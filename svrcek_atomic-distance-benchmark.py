import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



import matplotlib.pyplot as plt

import seaborn as sns

sns.set()



import os

print(os.listdir("../input"))
from sklearn.model_selection import GroupKFold

from sklearn.preprocessing import LabelEncoder

from sklearn.ensemble import RandomForestRegressor

train = pd.read_csv('../input/train.csv', index_col='id')

test = pd.read_csv('../input/test.csv', index_col='id')
display(train.head())
structures = pd.read_csv('../input/structures.csv')

display(structures.head())
# Map the atom structure data into train and test files



def map_atom_info(df, atom_idx):

    df = pd.merge(df, structures, how = 'left',

                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],

                  right_on = ['molecule_name',  'atom_index'])

    

    df = df.drop('atom_index', axis=1)

    df = df.rename(columns={'atom': f'atom_{atom_idx}',

                            'x': f'x_{atom_idx}',

                            'y': f'y_{atom_idx}',

                            'z': f'z_{atom_idx}'})

    return df



train = map_atom_info(train, 0)

train = map_atom_info(train, 1)



test = map_atom_info(test, 0)

test = map_atom_info(test, 1)
# Engineer a single feature: distance vector between atoms

#  (there's ways to speed this up!)



def dist(row):

    return ( (row['x_1'] - row['x_0'])**2 +

             (row['y_1'] - row['y_0'])**2 +

             (row['z_1'] - row['z_0'])**2 ) ** 0.5



train['dist'] = train.apply(lambda x: dist(x), axis=1)

test['dist'] = test.apply(lambda x: dist(x), axis=1)
molecules = train.pop('molecule_name')

test = test.drop('molecule_name', axis=1)



y = train.pop('scalar_coupling_constant')



# Label Encoding

for f in ['type', 'atom_0', 'atom_1']:

    lbl = LabelEncoder()

    lbl.fit(list(train[f].values) + list(train[f].values))

    train[f] = lbl.transform(list(train[f].values))

    test[f] = lbl.transform(list(test[f].values))
yoof = np.zeros(len(train))

yhat = np.zeros(len(test))



n_splits = 3

gkf = GroupKFold(n_splits=n_splits) # we're going to split folds by molecules



fold = 0

for in_index, oof_index in gkf.split(train, y, groups=molecules):

    fold += 1

    print(f'fold {fold} of {n_splits}')

    X_in, X_oof = train.values[in_index], train.values[oof_index]

    y_in, y_oof = y.values[in_index], y.values[oof_index]

    reg = RandomForestRegressor(n_estimators=250,

                                max_depth=9,

                                min_samples_leaf=3,

                                n_jobs=-1)

    reg.fit(X_in, y_in)

    yoof[oof_index] = reg.predict(X_oof)

    yhat += reg.predict(test)



yhat /= n_splits
sample_submission = pd.read_csv('../input/sample_submission.csv', index_col='id')



benchmark = sample_submission.copy()

benchmark['scalar_coupling_constant'] = yhat

benchmark.to_csv('atomic_distance_benchmark.csv')
plot_data = pd.DataFrame(y)

plot_data.index.name = 'id'

plot_data['yhat'] = yoof

plot_data['type'] = pd.read_csv('../input/train.csv', index_col='id', usecols=['id', 'type'])



def plot_oof_preds(ctype, llim, ulim):

        plt.figure(figsize=(6,6))

        sns.scatterplot(x='scalar_coupling_constant',y='yhat',

                        data=plot_data.loc[plot_data['type']==ctype,

                        ['scalar_coupling_constant', 'yhat']]);

        plt.xlim((llim, ulim))

        plt.ylim((llim, ulim))

        plt.plot([llim, ulim], [llim, ulim])

        plt.xlabel('scalar_coupling_constant')

        plt.ylabel('predicted')

        plt.title(f'{ctype}', fontsize=18)

        plt.show()



plot_oof_preds('1JHC', 0, 250)

plot_oof_preds('1JHN', 0, 100)

plot_oof_preds('2JHC', -50, 50)

plot_oof_preds('2JHH', -50, 50)

plot_oof_preds('2JHN', -25, 25)

plot_oof_preds('3JHC', -25, 100)

plot_oof_preds('3JHH', -20, 20)

plot_oof_preds('3JHN', -15, 15)