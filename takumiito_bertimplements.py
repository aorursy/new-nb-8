import sys

# copy our file into the working directory

sys.path.insert(0, "../input/pytorch-pretrained-BERT/pytorch-pretrained-BERT/pytorch-pretrained-bert/")
import torch

from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch

from pytorch_pretrained_bert import BertTokenizer, BertModel
batch_size = 256



n_seeds = 1

n_splits = 10

n_epochs = 15

EMBED_SIZE = 300

SUBGROUP_NEGATIVE_WEIGHT_COEF = 1

BACKGROUND_POSITIVE_WEIGHT_COEF = 0



ENSEMBLE_START_EPOCH = 3



MAX_LEN = 220



EMB_DROPOUT = 0.3

MIDDLE_DROPOUT = 0.3



BERT_MODEL_PATH = '../input/bert-pretrained-models/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'

# åˆ†æ•£è¡¨ç¾ãŒuncasedã ã£ãŸå ´åˆã¯ã€å˜èªã‚’å…¨ã¦å°æ–‡å­—ã§æ‰±ã†

BERT_DO_LOWER = 'uncased' in BERT_MODEL_PATH



WORK_DIR = "../working/"



batch_size = 32



OUT_DROPOUT = 0.3



BERT_HIDDEN_SIZE = 768
from contextlib import contextmanager

import os



# æ¨™æº–å‡ºåŠ›ã‚’nullã«å¤‰æ›

@contextmanager

def suppress_stdout():

    # nullã®æ›¸ãè¾¼ã¿

    with open(os.devnull, "w") as devnull:

        # æ¨™æº–å‡ºåŠ›ã‚’nullã«å¤‰æ›

        old_stdout = sys.stdout

        sys.stdout = devnull

        try:  

            yield

        finally:

            sys.stdout = old_stdout
import pandas as pd

raw_train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')

#test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')
import gc

AUX_COLUMNS = ['target', 'severe_toxicity','obscene','identity_attack','insult','threat']

train_data = raw_train[:100000]

train_text = train_data['comment_text']

y_target = (train_data['target'] >= 0.5).astype('float32')

#print(y_target)

y_aux_target = (train_data[AUX_COLUMNS] >= 0.5).astype('float32')

#x_test = test['comment_text'][:300]

#print(y_aux_target)

#print(len(y_target.values))

#print(y_aux_target.shape)

#y_target = torch.tensor(y_target.values.reshape((-1, 1)), dtype=torch.float32).cuda()

#y_aux_target = torch.tensor(y_aux_target.values, dtype=torch.float32).cuda()

y_label = pd.concat([y_target, y_aux_target], axis=1)

#print(y_label)

#print(train_text)
del raw_train, train_data, y_target, y_aux_target

gc.collect()
import re

import emoji

import unicodedata



# ç‰¹å®šæ–‡å­—ã®å¤‰æ›å™¨ã‚’ä½œæˆ

CUSTOM_TABLE = str.maketrans(

    {

        "\xad": None,

        "\x7f": None,

        "\x10": None,

        "\x9d": None,

        "\xa0": None,

        "\ufeff": None,

        "\u200b": None,

        "\u200e": None,

        "\u202a": None,

        "\u202c": None,

        "\uf0d8": None,

        "\u2061": None,

        "â€˜": "'",

        "â€™": "'",

        "`": "'",

        "â€œ": '"',

        "â€": '"',

        "Â«": '"',

        "Â»": '"',

        "É¢": "G",

        "Éª": "I",

        "É´": "N",

        "Ê€": "R",

        "Ê": "Y",

        "Ê™": "B",

        "Êœ": "H",

        "ÊŸ": "L",

        "Ò“": "F",

        "á´€": "A",

        "á´„": "C",

        "á´…": "D",

        "á´‡": "E",

        "á´Š": "J",

        "á´‹": "K",

        "á´": "M",

        "Îœ": "M",

        "á´": "O",

        "á´˜": "P",

        "á´›": "T",

        "á´œ": "U",

        "á´¡": "W",

        "á´ ": "V",

        "Ä¸": "K",

        "Ğ²": "B",

        "Ğ¼": "M",

        "Ğ½": "H",

        "Ñ‚": "T",

        "Ñ•": "S",

        "â€”": "-",

        "â€“": "-",

    }

)



# ä¸‹å“ãªå˜èªã®è¦åˆ¶å¾Œã¨è¦åˆ¶å‰ã®å˜èªã®è¾æ›¸ã‚’ä½œæˆ

WORDS_REPLACER = [

    ("sh*t", "shit"),

    ("s**t", "shit"),

    ("f*ck", "fuck"),

    ("fu*k", "fuck"),

    ("f**k", "fuck"),

    ("f*****g", "fucking"),

    ("f***ing", "fucking"),

    ("f**king", "fucking"),

    ("p*ssy", "pussy"),

    ("p***y", "pussy"),

    ("pu**y", "pussy"),

    ("p*ss", "piss"),

    ("b*tch", "bitch"),

    ("bit*h", "bitch"),

    ("h*ll", "hell"),

    ("h**l", "hell"),

    ("cr*p", "crap"),

    ("d*mn", "damn"),

    ("stu*pid", "stupid"),

    ("st*pid", "stupid"),

    ("n*gger", "nigger"),

    ("n***ga", "nigger"),

    ("f*ggot", "faggot"),

    ("scr*w", "screw"),

    ("pr*ck", "prick"),

    ("g*d", "god"),

    ("s*x", "sex"),

    ("a*s", "ass"),

    ("a**hole", "asshole"),

    ("a***ole", "asshole"),

    ("a**", "ass"),

]



# ä¸‹å“ãªå˜èªã®è¦åˆ¶éƒ¨åˆ†ã®ç‰¹æ®Šæ–‡å­—ã‚’ç„¡åŠ¹åŒ–ã—ã€å¤§æ–‡å­—å°æ–‡å­—ã‚’åŒºåˆ¥ã—ãªã„åˆ¤åˆ¥å™¨ã‚’ä½œæˆ

REGEX_REPLACER = [

    (re.compile(pat.replace("*", "\*"), flags=re.IGNORECASE), repl)

    for pat, repl in WORDS_REPLACER

]



# ç©ºç™½ã®åˆ¤åˆ¥å™¨ã‚’ä½œæˆ

RE_SPACE = re.compile(r"\s")

RE_MULTI_SPACE = re.compile(r"\s+")



# Unicodeã‹ã‚‰ç•°ä½“å­—ã‚»ãƒ¬ã‚¯ã‚¿ï¼ˆç›´å‰ã®æ–‡å­—ã®ç¨®é¡ã‚’å¤‰ãˆã‚‹ã‚³ãƒ¼ãƒ‰ï¼‰ã‚’keyã¨ã™ã‚‹è¾æ›¸ã‚’ä½œæˆ

NMS_TABLE = dict.fromkeys(

    i for i in range(sys.maxunicode + 1) if unicodedata.category(chr(i)) == "Mn"

)



# ç‰¹å®šã®Unicodeï¼ˆå­¦ç¿’ã§ä½¿ãˆãªã„æ–‡å­—ï¼Ÿï¼‰ã‚’åˆ¥ã®æ–‡å­—ã§ç½®æ›ã™ã‚‹è¾æ›¸ä½œæˆ

HEBREW_TABLE = {i: "×" for i in range(0x0590, 0x05FF)}

ARABIC_TABLE = {i: "Ø§" for i in range(0x0600, 0x06FF)}

CHINESE_TABLE = {i: "æ˜¯" for i in range(0x4E00, 0x9FFF)}

KANJI_TABLE = {i: "ãƒƒ" for i in range(0x2E80, 0x2FD5)}

HIRAGANA_TABLE = {i: "ãƒƒ" for i in range(0x3041, 0x3096)}

KATAKANA_TABLE = {i: "ãƒƒ" for i in range(0x30A0, 0x30FF)}



TABLE = dict()

TABLE.update(CUSTOM_TABLE)

TABLE.update(NMS_TABLE)

# Non-english languages

TABLE.update(CHINESE_TABLE)

TABLE.update(HEBREW_TABLE)

TABLE.update(ARABIC_TABLE)

TABLE.update(HIRAGANA_TABLE)

TABLE.update(KATAKANA_TABLE)

TABLE.update(KANJI_TABLE)



# çµµæ–‡å­—ã®åˆ¤åˆ¥å™¨ã‚’ä½œæˆ

EMOJI_REGEXP = emoji.get_emoji_regexp()



# çµµæ–‡å­—ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æ–‡ç« åŒ–

UNICODE_EMOJI_MY = {

    k: f" EMJ {v.strip(':').replace('_', ' ')} "

    for k, v in emoji.UNICODE_EMOJI_ALIAS.items()

}



# æ–‡ç« å†…ã®çµµæ–‡å­—ã‚’ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«å¤‰æ›

def my_demojize(string: str) -> str:

    # subé–¢æ•°ã§matchã—ãŸå˜èªã‚’ã€ãã‚Œã‚’å«ã‚€çµµæ–‡å­—ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«ç½®æ›

    def replace(match):

        return UNICODE_EMOJI_MY.get(match.group(0), match.group(0))



    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ–‡ç« ä¸­ã®çµµæ–‡å­—ã‚’ã€çµµæ–‡å­—ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«ç½®æ›ã—ã€ãã“ã‹ã‚‰ç•°ä½“å­—ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å‰Šé™¤

    return re.sub("\ufe0f", "", EMOJI_REGEXP.sub(replace, string))



# æ–‡ç« ä¸­ã®ç‰¹å®šã®ç‰¹æ®Šå˜èªã‚’è§£æç”¨ã«å¤‰æ›

def normalize(text: str) -> str:

    #text_len = len(text)

    #print(text)

    # æ–‡ç« å†…ã®çµµæ–‡å­—ã‚’ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã®æ–‡ç« ã«å¤‰æ›

    text = my_demojize(text)



    # ç©ºç™½ã‚’å…¨ã¦ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›

    text = RE_SPACE.sub(" ", text)

    # æ–‡å­—è¡¨ç¾ï¼ˆUnicodeï¼‰ã®è¦æ ¼ã‚’çµ±ä¸€åŒ–

    text = unicodedata.normalize("NFKD", text)

    # æ–‡ç« ä¸­ã®TABLE_keyã‚’valueã«å¤‰æ›

    text = text.translate(TABLE)

    # é€£ç¶šã—ãŸç©ºç™½ã‚’ä¸€ã¤ã®ã‚¹ãƒšãƒ¼ã‚¹ã«å¤‰æ›ã—ã€æ–‡ç« ã®ä¸¡ç«¯ã®ç©ºç™½ã‚’å‰Šé™¤

    text = RE_MULTI_SPACE.sub(" ", text).strip()



    # ä¸‹å“ãªå˜èªã®è¦åˆ¶ã•ã‚ŒãŸéƒ¨åˆ†ã‚’ä¿®å¾©

    for pattern, repl in REGEX_REPLACER:

        text = pattern.sub(repl, text)

        

    #if text_len != len(text):

        #print(text + "\n")

    

    return text



# æ–‡ç« ã®ç‰¹æ®Šå˜èªã‚’ä¸€èˆ¬å˜èªã®å¤‰æ›

train_text = train_text.apply(lambda x: normalize(x))
del UNICODE_EMOJI_MY, EMOJI_REGEXP, TABLE, HEBREW_TABLE, ARABIC_TABLE, CHINESE_TABLE, KANJI_TABLE, HIRAGANA_TABLE, KATAKANA_TABLE, NMS_TABLE, RE_MULTI_SPACE, RE_SPACE, REGEX_REPLACER, WORDS_REPLACER, CUSTOM_TABLE

gc.collect()
#if "EMJ" in crawl_emb_dict:

#    print("EMJ")

#if "×" in crawl_emb_dict:

#    print("HEB")

#if "Ø§" in crawl_emb_dict:

#    print("ARA")

#if "æ˜¯" in crawl_emb_dict:

#    print("CHI")

#if "ãƒƒ" in crawl_emb_dict:

#    print("JAP")
puncts = [',', '.', '"', ':', ')', '(', '-', '!', '?', '|', ';', '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\', 'â€¢',  '~', '@', 'Â£', 

 'Â·', '_', '{', '}', 'Â©', '^', 'Â®', '`',  '<', 'â†’', 'Â°', 'â‚¬', 'â„¢', 'â€º',  'â™¥', 'â†', 'Ã—', 'Â§', 'â€³', 'â€²', 'Ã‚', 'â–ˆ', 'Â½', 'Ã ', 'â€¦', 

 'â€œ', 'â˜…', 'â€', 'â€“', 'â—', 'Ã¢', 'â–º', 'âˆ’', 'Â¢', 'Â²', 'Â¬', 'â–‘', 'Â¶', 'â†‘', 'Â±', 'Â¿', 'â–¾', 'â•', 'Â¦', 'â•‘', 'â€•', 'Â¥', 'â–“', 'â€”', 'â€¹', 'â”€', 

 'â–’', 'ï¼š', 'Â¼', 'âŠ•', 'â–¼', 'â–ª', 'â€ ', 'â– ', 'â€™', 'â–€', 'Â¨', 'â–„', 'â™«', 'â˜†', 'Ã©', 'Â¯', 'â™¦', 'Â¤', 'â–²', 'Ã¨', 'Â¸', 'Â¾', 'Ãƒ', 'â‹…', 'â€˜', 'âˆ', 

 'âˆ™', 'ï¼‰', 'â†“', 'ã€', 'â”‚', 'ï¼ˆ', 'Â»', 'ï¼Œ', 'â™ª', 'â•©', 'â•š', 'Â³', 'ãƒ»', 'â•¦', 'â•£', 'â•”', 'â•—', 'â–¬', 'â¤', 'Ã¯', 'Ã˜', 'Â¹', 'â‰¤', 'â€¡', 'âˆš', '\n', '\r', "'", "'", 'Î¸', 'Ã·', 'Î±', 'Î²', 'âˆ…', 'Ï€', 'â‚¹', 'Â´']



def clean_text(x: str) -> str:

    for punct in puncts:

        if punct in x:

            # ç‰¹å®šæ–‡å­—ã®ä¸¡å´ã«ç©ºç™½ä»˜ã‘ã‚‹

            x = x.replace(punct, ' {} '.format(punct))

    return x



import re

def clean_numbers(x):

    return re.sub('\d+', ' ', x)



# å¥èª­ç‚¹ã®ä¸¡å´ã«ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä»˜ä¸

train_text = train_text.apply(lambda x: clean_text(x))



# æ–‡ç« ã‹ã‚‰æ•°å­—å‰Šé™¤

train_text = train_text.apply(lambda x: clean_numbers(x))



# Bertã®èªå½™ã‚’èª­ã¿è¾¼ã¿

#with open(BERT_MODEL_PATH + 'vocab.txt', 'r') as f:

#    raw_dict = f.readlines()



# ãƒ‡ãƒ¼ã‚¿å†…ã®æ”¹è¡Œã‚’å‰Šé™¤ã¨é‡è¤‡è¡Œã®å‰Šé™¤

#crawl_emb_dict = set([t.replace('\n', '') for t in raw_dict])

#print(len(crawl_emb_dict))

    

import joblib

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆå˜èªã®åˆ†æ•£è¡¨ç¾ã®è¾æ›¸ï¼‰ã‚’ä¸¦åˆ—å‡¦ç†å½¢å¼ã§ä¿å­˜

with open('../input/reducing-oov-of-crawl300d2m-no-appos-result/jigsaw-crawl-300d-2M.joblib', 'rb') as f:

    crawl_emb_dict = joblib.load(f)

crawl_emb_dict = set(crawl_emb_dict.keys())

#print(crawl_emb_dict)

#for k in list(crawl_emb_dict)[:10]:

#    print({k:crawl_emb_dict[k]})



# ã‚°ãƒ¼ã‚°ãƒ«ãŒç¦æ­¢ã—ã¦ã„ã‚‹å˜èªï¼ˆç¦å¥ï¼‰é›†ã‚’å–å¾—

with open('../input/googleprofanitywords/google-profanity-words/google-profanity-words/profanity.js', 'r') as f:

    p_words = f.readlines()

    

set_puncts = set(puncts)

#print(set_puncts)



# ãƒ‡ãƒ¼ã‚¿å†…ã®æ”¹è¡Œã‚’å‰Šé™¤ã¨é‡è¤‡è¡Œã®å‰Šé™¤

p_word_set = set([t.replace('\n', '') for t in p_words])

#print(p_word_set)
del puncts, p_words#, raw_dict

gc.collect()
import operator

from typing import Dict, List

from tqdm import tqdm_notebook as tqdm



def check_coverage(texts, embeddings_word: Dict) -> List[str]:

    known_words = []

    unknown_words = []

    for text in tqdm(texts):

        text = text.split()

        for word in text:

            if word in embeddings_word:

                known_words.append(word)

                #print(word)

            else:

                unknown_words.append(word)



    print('Found embeddings for {:.2%} of vocab'.format(float(len(set(known_words))) / (float(len(set(known_words))) + float(len(set(unknown_words))))))

    print('Found embeddings for {:.2%} of all text'.format(float(len(known_words)) / (float(len(known_words)) + float(len(unknown_words)))))



    return set(unknown_words)



# ï¼“é€šã‚Šã®Stemmerï¼ˆæ¥å°¾è¾é™¤å»=èªå¹¹æ¤œå‡ºã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼‰ç”¨ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã—

from nltk.stem import PorterStemmer

p_stemmer = PorterStemmer()

from nltk.stem.lancaster import LancasterStemmer

l_stemmer = LancasterStemmer()

from nltk.stem import SnowballStemmer

s_stemmer = SnowballStemmer("english")



import copy

def edits1(word):

    """

    wordã®ç·¨é›†è·é›¢1ã®å˜èªã®ãƒªã‚¹ãƒˆã‚’è¿”ã™

    """

    letters    = 'abcdefghijklmnopqrstuvwxyz'

    # å˜èªã‚’å·¦å³ï¼’ã¤ã«åˆ†å‰²ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ

    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]

    # å˜èªã‹ã‚‰ä¸€æ–‡å­—ã‚’æ¶ˆã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ

    deletes    = [L + R[1:]               for L, R in splits if R]

    # å˜èªã‹ã‚‰éš£åŒå£«ã®æ–‡å­—ã‚’ä¸€çµ„å…¥ã‚Œæ›¿ãˆãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ

    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]

    # å˜èªå†…ã®ä¸€æ–‡å­—ã‚’åˆ¥ã®æ–‡å­—ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆä¸­ã®å…¨ã¦ã®æ–‡å­—ï¼‰ã«ãã‚Œãã‚Œå¤‰æ›ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ

    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]

    # å˜èªã®ã©ã“ã‹ã«åˆ¥ã®æ–‡å­—ï¼ˆã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆä¸­ã®å…¨ã¦ã®æ–‡å­—ï¼‰ã‚’åŠ ãˆãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½œæˆ

    inserts    = [L + c + R               for L, R in splits for c in letters]

    return set(deletes + transposes + replaces + inserts)



def known(words, embed): 

    "The subset of `words` that appear in the dictionary of WORDS."

    # å˜èªãŒè¾æ›¸å†…ã«å…¥ã£ã¦ã„ãŸã‚‰è¿”ã™

    return set(w for w in words if w in embed)



# å˜èªã®èª¤å­—è„±å­—ã‚’å¾©å…ƒ

def spellcheck(word, word_rank_dict):

    # å˜èªã«ä¸€æ–‡å­—å…¥ã‚ŒãŸã‚Šã€éš£åŒå£«ã®æ–‡å­—ã‚’å…¥ã‚Œæ›¿ãˆãŸã‚Šã—ãŸç‰©ã®å†…ã€å˜èªè¾æ›¸ã«ã‚ã‚‹ç‰©ã®ä¸­ã‹ã‚‰ã€ä¸€ç•ªæ–‡å­—æ•°ãŒå°‘ãªã„æ–‡å­—åˆ—ã‚’è¿”ã™

    return min(known(edits1(word), word_rank_dict), key=lambda word_rank_dict:word_rank_dict)





import unicodedata

punct_mapping = {"â€˜": "'", "â‚¹": "e", "Â´": "'", "Â°": "",

                 "â‚¬": "e", "â„¢": "tm", "âˆš": " sqrt ", "Ã—": "x",

                 "Â²": "2", "â€”": "-", "â€“": "-", "â€™": "'",

                 "_": "-", "`": "'", 'â€œ': '"', 'â€': '"',

                 'â€œ': '"', "Â£": "e", 'âˆ': 'infinity',

                 'Î¸': 'theta', 'Ã·': '/', 'Î±': 'alpha',

                 'â€¢': '.', 'Ã ': 'a', 'âˆ’': '-', 'Î²': 'beta',

                 'âˆ…': '', 'Â³': '3', 'Ï€': 'pi', 'SB91':'senate bill','tRump':'trump','utmterm':'utm term','FakeNews':'fake news','GÊ€á´‡at':'great','Ê™á´á´›toá´':'bottom','washingtontimes':'washington times','garycrum':'gary crum','htmlutmterm':'html utm term','RangerMC':'car','TFWs':'tuition fee waiver','SJWs':'social justice warrior','Koncerned':'concerned','Vinis':'vinys','Yá´á´œ':'you','Trumpsters':'trump','Trumpian':'trump','bigly':'big league','Trumpism':'trump','Yoyou':'you','Auwe':'wonder','Drumpf':'trump','utmterm':'utm term','Brexit':'british exit','utilitas':'utilities','á´€':'a', 'ğŸ˜‰':'wink','ğŸ˜‚':'joy','ğŸ˜€':'stuck out tongue', 'theguardian':'the guardian','deplorables':'deplorable', 'theglobeandmail':'the globe and mail', 'justiciaries': 'justiciary','creditdation': 'Accreditation','doctrne':'doctrine','fentayal': 'fentanyl','designation-': 'designation','CONartist' : 'con-artist','Mutilitated' : 'Mutilated','Obumblers': 'bumblers','negotiatiations': 'negotiations','dood-': 'dood','irakis' : 'iraki','cooerate': 'cooperate','COx':'cox','racistcomments':'racist comments','envirnmetalists': 'environmentalists',}

def process_stemmer(texts, embed):

    oov_word_set = set()

    new_texts = []

    for text in tqdm(texts):

        text = text.split()

        new_text = ""

        for word in text:

            #print(word)

            # åˆ†æ•£è¡¨ç¾è¾æ›¸ã‹ã‚‰è‰²ã€…ãªè¡¨ç¾ã§å¤‰æ›ã—ãŸæ–‡ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

            if word in embed:

                new_text += word + " "

                #print("embed:",word)

                continue



            # å˜èªã‚’å…¨ã¦å°æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

            if word.lower() in embed:

                new_text += word.lower() + " "

                #print("lower:",word)

                continue



            # å˜èªã‚’å…¨ã¦å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

            if word.upper() in embed:

                new_text += word.upper() + " "

                #print("upper:",word)

                continue



            # å˜èªã®é ­æ–‡å­—ã ã‘ã‚’å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

            if word.capitalize() in embed:

                new_text += word.capitalize() + " "

                #print("cap:",word)

                continue



            # ç‰¹æ®Šæ–‡å­—ã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

            corr_word = punct_mapping.get(word, None)

            if corr_word is not None:

                new_text += corr_word + " "

                #print("punct:",word)

                continue



            try:

                # PorterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                vector = p_stemmer.stem(word)

            except:

                # å¤±æ•—ã—ãŸã‚‰ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œ

                vector = p_stemmer.stem(word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("p_st:",vector)

                continue

                

            try:

                # LancasterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                vector = l_stemmer.stem(word)

            except:

                vector = l_stemmer.stem(word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("l_st:",vector)

                continue



            try:

                # SnowballStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                vector = s_stemmer.stem(word)

            except:

                vector = s_stemmer.stem(word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("s_st:",vector)

                continue



            # å˜èªã®åˆ†æ•£è¡¨ç¾ãŒæ¤œç´¢ã§ããªã‹ã£ãŸå˜èªã‚’è¨˜éŒ²

            oov_word_set.add(word)

            new_text += word + " "

            #print("oov:",word)

        if len(new_text.strip()) == 0:

            print(text)

            print("0:None!")

            new_text += "0" + " "

            

        new_texts.append(new_text.strip())

            

        #print(new_texts)

    return new_texts, oov_word_set



def process_small_capital(texts, embed, oov_set):

    oov_word_set = set()

    new_texts = []

    for text in tqdm(texts):

        text = text.split()

        new_text = ""

        for word in text:

            #print(word)

            if word not in oov_set:

                new_text += word + " "

                continue



            char_list = []

            any_small_capitial = False

            # å˜èªã‚’ä¸€æ–‡å­—ãšã¤å–å¾—

            for char in word:

                try:

                    # æ–‡å­—ã«å‰²ã‚ŠæŒ¯ã‚‰ã‚Œã¦ã„ã‚‹åå‰ï¼ˆ"a"="LATIN SMALL LETTER A"ï¼‰ã‚’å–å¾—

                    uni_name = unicodedata.name(char)

                except ValueError:

                    continue



                # æ–‡å­—ãŒãƒ©ãƒ†ãƒ³æ–‡å­—ã ã£ãŸå ´åˆ

                if 'LATIN SMALL LETTER' or 'LATIN CAPITAL LETTER' in uni_name:

                    # æ–‡å­—ã«å‰²ã‚ŠæŒ¯ã‚‰ã‚ŒãŸåå‰ã®æœ€å¾Œï¼ˆæ–‡å­—ãŒå¤§æ–‡å­—åŒ–ã—ãŸç‰©ï¼‰ã‚’å–å¾—

                    char = uni_name[-1]

                    any_small_capitial = True

                # ã‚­ãƒªãƒ«æ–‡å­—ã®"Ò“"ã¯ã€"F"ã«å¤‰æ›

                if 'CYRILLIC SMALL LETTER GHE WITH STROKE' in uni_name:

                    char = 'F'

                    any_small_capitial = True



                char_list.append(char)



            # å˜èªå†…ã®å…¨ã¦ã®æ–‡å­—ãŒã€åå‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã€ãƒ©ãƒ†ãƒ³æ–‡å­—ã§ã¯ãªãã€ã‚­ãƒªãƒ«æ–‡å­—ã®'Ò“'ã§ã‚‚ãªã„å ´åˆ

            if not any_small_capitial:

                oov_word_set.add(word)

                new_text += word + " "

                #print("oov_small_cap:",word)

                continue



            # å¤‰æ›ã—ãŸæ–‡å­—ã‚’ä¸€ã¤ã®å˜èªã«æˆ»ã™

            legit_word = ''.join(char_list)



            if legit_word in embed:

                new_text += legit_word + " "

                #print("embed:",legit_word)

                continue



            if legit_word.lower() in embed:

                new_text += legit_word.lower() + " "

                #print("lower:",legit_word)

                continue



            if legit_word.upper() in embed:

                new_text += legit_word.upper() + " "

                #print("upper:",legit_word)

                continue



            if legit_word.capitalize() in embed:

                new_text += legit_word.capitalize() + " "

                #print("cap:",legit_word)

                continue



            corr_word = punct_mapping.get(legit_word, None)

            if corr_word is not None:

                new_text += corr_word + " "

                #print("punct:",legit_word)

                continue



            try:

                vector = p_stemmer.stem(legit_word)

            except:

                vector = p_stemmer.stem(legit_word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("p_st:",vector)

                continue



            try:

                vector = l_stemmer.stem(legit_word)

            except:

                vector = l_stemmer.stem(legit_word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("l_st:",vector)

                continue



            try:

                vector = s_stemmer.stem(legit_word)

            except:

                vector = s_stemmer.stem(legit_word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("s_st:",vector)

                continue



            oov_word_set.add(word)

            new_text += word + " "

            #print("oov:",word)

            

        if len(new_text.strip()) == 0:

            print(text)

            print("0:None!")

            new_text += "0" + " "



        new_texts.append(new_text.strip())

        

        #print(new_texts)

    return new_texts, oov_word_set



def process_spellcheck(texts, embed, oov_set):

    oov_word_set = set()

    new_texts = []

    for text in tqdm(texts):

        text = text.split()

        new_text = ""

        for word in text:

            if word not in oov_set:

                new_text += word + " "

                continue



            try:

                vector = spellcheck(word, embed)

            except:

                oov_word_set.add(word)

                new_text += word + " "

                #print("oov:",word)

                continue

            if vector is not None:

                new_text += vector + " "

                #print("original:",word)

                #print("miss_sp:",vector)

                continue



            oov_word_set.add(word)

            new_text += word + " "

            #print("oov:",word)

            

        if len(new_text.strip()) == 0:

            print(text)

            print("0:None!")

            new_text += "0" + " "

            

        new_texts.append(new_text.strip())

            

        #print(new_texts)

    return new_texts, oov_word_set



from nltk import TweetTokenizer

# Tweetå°‚ç”¨ã®è§£æãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆ

# reduce_lenï¼šå˜èªã®é•·ã•ã®æ¨™æº–åŒ–ï¼ˆçŸ­ç¸®åŒ–ï¼‰ã‚’ã™ã‚‹ã‹ã©ã†ã‹ã‚’è¨­å®š

tknzr = TweetTokenizer(reduce_len=True)

def twitter_stemmer(texts, embed, oov_set):

    oov_word_set = set()

    new_texts = []

    for text in tqdm(texts):

        text = text.split()

        new_text = ""

        for word in text:

            if word not in oov_set:

                new_text += word + " "

                continue

            

            tokens = tknzr.tokenize(word)

            if (tokens[0] == "'" or '"') and len(tokens) > 1:

                word = tokens[1]

                #print("tokens_1:")

            else:

                word = tokens[0]

                #print("tokens_0:")

            #print(word)

            # åˆ†æ•£è¡¨ç¾è¾æ›¸ã‹ã‚‰è‰²ã€…ãªè¡¨ç¾ã§å¤‰æ›ã—ãŸæ–‡ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

            if word in embed:

                new_text += word + " "

                #print("embed:",word)

                continue



            # å˜èªã‚’å…¨ã¦å°æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

            if word.lower() in embed:

                new_text += word.lower() + " "

                #print("lower:",word)

                continue



            # å˜èªã‚’å…¨ã¦å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

            if word.upper() in embed:

                new_text += word.upper() + " "

                #print("upper:",word)

                continue



            # å˜èªã®é ­æ–‡å­—ã ã‘ã‚’å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

            if word.capitalize() in embed:

                new_text += word.capitalize() + " "

                #print("cap:",word)

                continue



            # ç‰¹æ®Šæ–‡å­—ã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

            corr_word = punct_mapping.get(word, None)

            if corr_word is not None:

                new_text += corr_word + " "

                #print("punct:",word)

                continue



            try:

                # PorterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                vector = p_stemmer.stem(word)

            except:

                # å¤±æ•—ã—ãŸã‚‰ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œ

                vector = p_stemmer.stem(word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("p_st:",vector)

                continue

                

            try:

                # LancasterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                vector = l_stemmer.stem(word)

            except:

                vector = l_stemmer.stem(word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("l_st:",vector)

                continue



            try:

                # SnowballStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                vector = s_stemmer.stem(word)

            except:

                vector = s_stemmer.stem(word.decode('utf-8'))

            if vector in embed:

                new_text += vector + " "

                #print("s_st:",vector)

                continue



            # å˜èªã®åˆ†æ•£è¡¨ç¾ãŒæ¤œç´¢ã§ããªã‹ã£ãŸå˜èªã‚’è¨˜éŒ²

            oov_word_set.add(word)

            new_text += word + " "

            #print("oov:",word)

        if len(new_text.strip()) == 0:

            print("0:None!")

            print(text)

            new_text += "0" + " "

            

        new_texts.append(new_text.strip())

            

        #print(new_texts)

    return new_texts, oov_word_set



def bytes_to_unicode():

    # ãƒ©ãƒ†ãƒ³æ–‡å­—ã‚’è¡¨ã™Unicodeã‚’å–å¾—

    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("Â¡"), ord("Â¬")+1))+list(range(ord("Â®"), ord("Ã¿")+1))

    cs = bs[:]

    n = 0

    # 0~255ã®Unicodeã‚’å–å¾—

    for b in range(2**8):

        if b not in bs:

            # æ–‡å­—ä»¥å¤–ã®Unicodeï¼ˆPCã®å‘½ä»¤ã‚³ãƒ¼ãƒ‰ï¼‰ã‚‚å–å¾—

            bs.append(b)

            # ãƒ©ãƒ†ãƒ³æ–‡å­—æ‹¡å¼µAï¼ˆPCã®å‘½ä»¤ã‚³ãƒ¼ãƒ‰+256ã«ä½ç½®ã™ã‚‹æ–‡å­—ï¼‰ã®Unicodeã‚’å–å¾—

            cs.append(2**8+n)

            n += 1

    # é…åˆ—å†…ã®Unicodeã‚’å˜èªã«å¤‰æ›

    cs = [chr(n) for n in cs]

    return dict(zip(bs, cs))



# å˜èªã®å…ˆé ­æ–‡å­—ã‹ã‚‰äºŒæ–‡å­—ãšã¤ã®ã‚¿ãƒ—ãƒ«ã‚’å–å¾—

def get_pairs(word):

    pairs = set()

    prev_char = word[0]

    for char in word[1:]:

        pairs.add((prev_char, char))

        prev_char = char

    return pairs



# ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰mergeæ–‡å­—å–å¾—

MERGES_PATH = "../input/transformer-tokenizers/gpt2/merges.txt"

bpe_data = open(MERGES_PATH, encoding='utf-8').read().split('\n')[1:-1]

# mergeæ–‡å­—ã®firstã¨secondã‚’ã‚¿ãƒ—ãƒ«åŒ–

bpe_merges = [tuple(merge.split()) for merge in bpe_data]

# keyãŒmergeæ–‡å­—ã®ã‚¿ãƒ–ãƒ«ã€valueãŒ0ã‹ã‚‰ã®idã®è¾æ›¸ä½œæˆ

bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))

def bpe(token):

    word = tuple(token)

    # å˜èªã®å…ˆé ­æ–‡å­—ã‹ã‚‰äºŒæ–‡å­—ãšã¤ã®ã‚¿ãƒ—ãƒ«ã‚’å–å¾—

    pairs = get_pairs(word)



    if not pairs:

        return token



    while True:

        # å˜èªä¸­ã§ãƒãƒƒãƒã—ãŸmergeæ–‡å­—ã®ã‚¿ãƒ—ãƒ«ã‹ã‚‰ã€idãŒä¸€ç•ªå°ã•ã„ã‚‚ã®ã‚’å–å¾—

        bigram = min(pairs, key = lambda pair: bpe_ranks.get(pair, float('inf')))

        if bigram not in bpe_ranks:

            break

        first, second = bigram

        new_word = []

        i = 0

        while i < len(word):

            try:

                # iç•ªç›®ã®æ–‡å­—ã‹ã‚‰å¾Œã‚ã®mergeæ–‡å­—ã®firstã®idã‚’å–å¾—

                j = word.index(first, i)

                # iç•ªç›®ã®æ–‡å­—ã‹ã‚‰mergeæ–‡å­—ã®firstã®ç›´å‰ã¾ã§ã®æ–‡å­—åˆ—ã‚’å–å¾—

                new_word.extend(word[i:j])

                i = j

            except:

                # mergeæ–‡å­—ã®firstãŒå˜èªã«å«ã¾ã‚Œã¦ãªã„å ´åˆã¯ã€ãã®ã¾ã¾å–å¾—

                new_word.extend(word[i:])

                break



            # mergeæ–‡å­—ã®firstã¨secondã®äºŒæ–‡å­—ãŒé€£ç¶šã§ç¶šã„ãŸæ™‚ã‚‚å–å¾—

            if word[i] == first and i < len(word)-1 and word[i+1] == second:

                new_word.append(first+second)

                i += 2

            else:

                new_word.append(word[i])

                i += 1

        new_word = tuple(new_word)

        word = new_word

        #print("new_word:",new_word)

        if len(word) == 1:

            break

        else:

            pairs = get_pairs(word)

    word = ' '.join(word)

    return word



def merge_spellcheck(texts, embed, oov_set):

    # keyï¼ˆãƒ©ãƒ†ãƒ³æ–‡å­—ã¨PCå‘½ä»¤ã‚³ãƒ¼ãƒ‰ï¼‰ã€valueï¼ˆãƒ©ãƒ†ãƒ³æ–‡å­—ã¨ãƒ©ãƒ†ãƒ³æ–‡å­—æ‹¡å¼µï¼‰ã§ã‚ã‚‹è¾æ›¸ã‚’å–å¾—

    byte_encoder = bytes_to_unicode()

    # æ–‡ç« ã‚’å˜èªï¼ˆè§£æã™ã‚‹ãƒ‘ãƒ¼ãƒ„ï¼‰ã«åˆ†ã‘ã‚‹æ–‡å­—ã®åˆ¤åˆ¥å™¨

    #pat = re.compile(r"""'s|'t|'re|'ve|'m|'ll|'d|[^('s)('t)('re)('ve)('m)('ll)('d)]+""")

    oov_word_set = set()

    new_texts = []

    for text in tqdm(texts):

        text = text.split()

        new_text = ""

        for pre_word in text:

            if pre_word not in oov_set:

                new_text += pre_word + " "

                continue

            #print("original:",pre_word)

            bpe_tokens = []

            token_list = re.split("('s)|('t)|('re)|('ve)|('m)|('ll)|('d)", pre_word)

            token_list = [x for x in token_list if (x is not None) and (x != "")]

            #print("token_list:",token_list)

            # å˜èªä¸­ã®å˜èªã‚’mergeæ–‡å­—ã§ç´°åˆ†åŒ–

            for token in token_list:

                #print("token:",token)

                # å˜èªä¸­ã®ãƒ©ãƒ†ãƒ³æ–‡å­—ã¨PCå‘½ä»¤ã‚³ãƒ¼ãƒ‰ã‚’ãƒ©ãƒ†ãƒ³æ–‡å­—æ‹¡å¼µAã«å¤‰æ›ã—ãŸæ–‡å­—ã®ã¿ã®å˜èªã‚’ç”Ÿæˆ

                token = ''.join(byte_encoder[b] for b in token.encode('utf-8'))

                # mergeæ–‡å­—ã«ã‚ˆã£ã¦åˆ†ã‹ã¡æ›¸ãã—ãŸå˜èªã‚’ä¿å­˜

                bpe_tokens.extend(bpe_token for bpe_token in bpe(token).split(' '))

                if (len(bpe_tokens) == 1) and (bpe_tokens[0] == token) and (len(token_list) == 0):

                    oov_word_set.add(word)

                    new_text += word + " "

                    #print("no_tokens:")

                    continue

                if len(bpe_tokens) == 1:

                    bpe_tokens.append(" ")



                for word in bpe_tokens:

                    #print("word:",word)

                    # åˆ†æ•£è¡¨ç¾è¾æ›¸ã‹ã‚‰è‰²ã€…ãªè¡¨ç¾ã§å¤‰æ›ã—ãŸæ–‡ä¸­ã®å˜èªã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

                    if word in embed:

                        new_text += word + " "

                        #print("embed:",word)

                        continue



                    # å˜èªã‚’å…¨ã¦å°æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

                    if word.lower() in embed:

                        new_text += word.lower() + " "

                        #print("lower:",word)

                        continue



                    # å˜èªã‚’å…¨ã¦å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

                    if word.upper() in embed:

                        new_text += word.upper() + " "

                        #print("upper:",word)

                        continue



                    # å˜èªã®é ­æ–‡å­—ã ã‘ã‚’å¤§æ–‡å­—åŒ–ã—ãŸç‰©ã§æ¤œç´¢

                    if word.capitalize() in embed:

                        new_text += word.capitalize() + " "

                        #print("cap:",word)

                        continue



                    # ç‰¹æ®Šæ–‡å­—ã®åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

                    corr_word = punct_mapping.get(word, None)

                    if corr_word is not None:

                        new_text += corr_word + " "

                        #print("punct:",word)

                        continue



                    try:

                        # PorterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                        vector = p_stemmer.stem(word)

                    except:

                        # å¤±æ•—ã—ãŸã‚‰ã€æ–‡å­—ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã—ã¦å†å®Ÿè¡Œ

                        vector = p_stemmer.stem(word.decode('utf-8'))

                    if vector in embed:

                        new_text += vector + " "

                        #print("p_st:",vector)

                        continue



                    try:

                        # LancasterStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                        vector = l_stemmer.stem(word)

                    except:

                        vector = l_stemmer.stem(word.decode('utf-8'))

                    if vector in embed:

                        new_text += vector + " "

                        #print("l_st:",vector)

                        continue



                    try:

                        # SnowballStemmerã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æŠ½å‡ºã—ãŸå˜èªã®èªå¹¹ã§æ¤œç´¢

                        vector = s_stemmer.stem(word)

                    except:

                        vector = s_stemmer.stem(word.decode('utf-8'))

                    if vector in embed:

                        new_text += vector + " "

                        #print("s_st:",vector)

                        continue



                    # å˜èªã®åˆ†æ•£è¡¨ç¾ãŒæ¤œç´¢ã§ããªã‹ã£ãŸå˜èªã‚’è¨˜éŒ²

                    oov_word_set.add(word)

                    new_text += word + " "

                    #print("oov:",word)

        if len(new_text.strip()) == 0:

            print(text)

            print("0:None!")

            new_text += "0" + " "

            

        new_texts.append(new_text.strip())

            

        #print(new_texts)

    return new_texts, oov_word_set



def head(enumerable, n=10):

    #print(enumerable)

    for i, item in enumerate(enumerable):

        print(str(i) + '\n',item)

        if i > n:

            return



print("only_data:")

# åˆ†æ•£è¡¨ç¾ã®è¾æ›¸ã«ç™»éŒ²ã•ã‚Œã¦ã„ãªã„å˜èªã‚’å–å¾—

oov = check_coverage(train_text, crawl_emb_dict)

head(oov)



# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å˜èªã«æ¥å°¾è¾é™¤å»ã‚’ç”¨ã„ã¦ã€åˆ†æ•£è¡¨ç¾ã®è¾æ›¸ä½œæˆ

train_text, oov_stemer = process_stemmer(train_text, crawl_emb_dict)

#print(train_text)

print("stemmer_data:")

oov = check_coverage(train_text, crawl_emb_dict)

head(oov)

#print(oov_stemer)



# å˜èªä¸­ã®æ–‡å­—ã‚’è§£æã—ã¦ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

train_text, oov_small_capital = process_small_capital(train_text, crawl_emb_dict, oov_stemer)

print("small_capital_data:")

oov = check_coverage(train_text, crawl_emb_dict)

head(oov)

#print(oov_small_capital)



# å˜èªã®èª¤å­—è„±å­—ã‚’è§£æã—ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

#train_text_test = train_text

train_text, oov_spell = process_spellcheck(train_text, crawl_emb_dict, oov_small_capital)

print("spellcheck_data:")

oov = check_coverage(train_text, crawl_emb_dict)

head(oov)

#print(oov_spell)



# twitterã®åˆ†ã‹ã¡æ›¸ãã‚’ä½¿ã£ã¦ã€å˜èªã‚’å¤‰æ›ã—ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

#train_text_test = train_text

train_text, oov_twitter = twitter_stemmer(train_text, crawl_emb_dict, oov_spell)

print("twitter_stemmer_data:")

oov = check_coverage(train_text, crawl_emb_dict)

head(oov)

#print(oov_twitter)



# mergeæ–‡å­—ã‚’ä½¿ã£ã¦ã€å˜èªã‚’å¤‰æ›ã—ã€åˆ†æ•£è¡¨ç¾ã‚’æ¤œç´¢

#train_text_test = train_text

train_text, oov_mearge = merge_spellcheck(train_text, crawl_emb_dict, oov_twitter)

print("merge_data:")

oov = check_coverage(train_text, crawl_emb_dict)

head(oov)
del oov, oov_stemer, oov_small_capital, oov_spell, oov_mearge, oov_twitter, bpe_data, bpe_merges, bpe_ranks

gc.collect()
# æ–‡ç« ã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ

def sentence_fetures(text):

    word_list = text.split()

    #print(word_list)

    # å˜èªæ•°

    word_count = len(word_list)

    # å¤§æ–‡å­—ã‚’å«ã‚€å˜èªã®æ•°

    n_upper = len([word for word in word_list if any([c.isupper() for c in word])])

    # å«ã¾ã‚Œã‚‹å˜èªã®ç¨®é¡

    n_unique = len(set(word_list))

    # ãƒ“ãƒƒã‚¯ãƒªãƒãƒ¼ã‚¯ã®æ•°

    n_ex = word_list.count('!')

    #print(n_ex)

    # ã‚¯ã‚¨ã‚¹ãƒãƒ§ãƒ³ãƒãƒ¼ã‚¯ã®æ•°

    n_que = word_list.count('?')

    # ç‰¹æ®Šæ–‡å­—ï¼ˆå¥èª­ç‚¹ï¼‰ã®æ•°

    n_puncts = len([word for word in word_list if word in set_puncts])

    # ç¦å¥ã®æ•°

    n_prof = len([word for word in word_list if word in p_word_set])

    # unknownå˜èªã®æ•°

    n_oov = len([word for word in word_list if word not in crawl_emb_dict])

    

    return word_count, n_upper, n_unique, n_ex, n_que, n_puncts, n_prof, n_oov



from collections import defaultdict

sentence_feature_cols = ['word_count', 'n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']

# keyä¸è¦ã®valueãŒlistå‹ã®è¾æ›¸ã‚’ä½œæˆ

feature_dict = defaultdict(list)

#print(raw_train)

for text in train_text:

    #print(text)

    # æ–‡ç« ã®ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

    feature_list = sentence_fetures(text)

    for i_feature, feature_name in enumerate(sentence_feature_cols):

        # keyã‚’ç‰¹å¾´ã®åå‰ã€valueã‚’æ–‡ç« ã®ç‰¹å¾´å€¤ã¨ã—ãŸè¾æ›¸ã‚’ä½œæˆ

        feature_dict[sentence_feature_cols[i_feature]].append(feature_list[i_feature])

        

sentence_df = pd.DataFrame.from_dict(feature_dict)

#print(sentence_df['word_count'])

# å„ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’å˜èªã®æ•°ã§å¹³å‡åŒ–

for col in ['n_upper', 'n_unique', 'n_ex', 'n_que', 'n_puncts', 'n_prof', 'n_oov']:

    sentence_df[col + '_ratio'] = sentence_df[col] / sentence_df['word_count']

#print(sentence_df)

    

# ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰unknownãƒ¯ãƒ¼ãƒ‰é–¢é€£ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤

sentence_feature_mat = sentence_df.drop(columns=['n_oov', 'n_oov_ratio']).values

#print(sentence_feature_mat)



# ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ã‚’æ¨™æº–åŒ–ï¼ˆå¹³å‡0ã€åˆ†æ•£1ã®é›†åˆåŒ–ï¼‰

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(sentence_feature_mat)

sentence_feature_mat = scaler.transform(sentence_feature_mat)

#print(sentence_feature_mat)

#print(scaler.var_)
del sentence_feature_cols, feature_dict, sentence_df, scaler

gc.collect()
#sentence_feature_mat = torch.tensor(sentence_feature_mat, dtype=torch.float32).cuda()

sentence_feature_size = sentence_feature_mat.shape[-1]

y_label_size = y_label.shape[-1]

#print(y_label_size)

#print(sentence_feature_mat.shape)

#print(sentence_feature_size)

import numpy as np

label_data = torch.tensor(np.concatenate([y_label, sentence_feature_mat], axis=1), dtype=torch.float32).cuda()

#print(y_target)

#print(y_target.shape)

n_samples = len(train_text)

train_size = int(n_samples * 0.4)

valid_size = int(n_samples * 0.3)

#train_index = list(range(0, train_size))

#valid_index = list(range(train_size, train_size + valid_size))

#test_index = list(range(train_size + valid_size, n_samples))

#print(n_samples)

train = pd.Series(list(train_text[0:train_size]))

train_label = label_data[0:train_size]

valid = pd.Series(list(train_text[train_size:train_size + valid_size]))

valid_label = label_data[train_size:train_size + valid_size]

test = pd.Series(list(train_text[train_size + valid_size:n_samples]))

test_label = label_data[train_size + valid_size:n_samples]

#print(len(train))

#print(len(valid))

#print(len(test))

#print(valid_label)

#print(label_data.shape)
del y_label, n_samples, train_size, valid_size, sentence_feature_mat#, train_index, valid_index, test_index

gc.collect()
tokenizer = BertTokenizer.from_pretrained(

    BERT_MODEL_PATH, cache_dir=None, do_lower_case=BERT_DO_LOWER)
# æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’æ•´åœ°

def tokenize(text, max_len, tokenizer):

    # vocab.txtã«ä¹—ã£ã¦ã„ã‚‹æ–‡å­—ã ã‘ã‚’æ–‡ç« å†…ã«å–å¾—

    tokenized_text = tokenizer.tokenize(text)[:max_len-2]

    return ["[CLS]"]+tokenized_text+["[SEP]"]



# ä»¥å‰ã®pandasãƒ•ã‚¡ã‚¤ãƒ«ã®é€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–

#import pandas as pd

#from tqdm import tqdm

#tqdm.pandas()

#from tqdm import tqdm_notebook as tqdm

#from tqdm._tqdm_notebook import tqdm_notebook

#tqdm_notebook.pandas()

train = train.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))

valid = valid.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))

test = test.apply(lambda x: tokenize(x, MAX_LEN, tokenizer))
# æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã‚’vocab.txtå†…ã®åˆ†æ•£è¡¨ç¾ã«å¤‰æ›

train = train.apply(lambda x: tokenizer.convert_tokens_to_ids(x))

valid = valid.apply(lambda x: tokenizer.convert_tokens_to_ids(x))

test = test.apply(lambda x: tokenizer.convert_tokens_to_ids(x))
del tokenizer

gc.collect()



torch.cuda.empty_cache()

torch.cuda.memory_allocated()
import shutil

# printãªã©ã®æ¨™æº–å‡ºåŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’éè¡¨ç¤º

with suppress_stdout():

    # äº‹å‰å­¦ç¿’æ¸ˆã¿ã®Bertãƒ¢ãƒ‡ãƒ«ï¼ˆTendorflowModelï¼‰ã‚’Pytorchä¸Šã«ä½œæˆ

    convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(

        # Bertï¼ˆTendorflowModelï¼‰ã®äº‹å‰å­¦ç¿’æ™‚ï¼ˆcheckpointï¼‰ã®é‡ã¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«è¨­å®š

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã§å‡ºåŠ›ã¯æ—¢ã«æ±ºå®šã•ã‚Œã¦ã„ã‚‹ï¼Ÿ

        # ãƒ•ã‚¡ã‚¤ãƒ«å†…ã®å¤‰æ•°åã«'squad'ãŒã‚ã‚‹æ™‚ã€äº‹å‰å­¦ç¿’ã®å‡ºåŠ›ã¯Classificationï¼ˆæœ€åˆã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã®å‡ºåŠ›ï¼‰ã«ãªã‚‹ï¼Ÿ

        BERT_MODEL_PATH + 'bert_model.ckpt',

        BERT_MODEL_PATH + 'bert_config.json',

        # é‡ã¿ã‚’è¨­å®šã—ãŸBERTãƒ¢ãƒ‡ãƒ«ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’WORK_DIRä¸Šã«ä¿å­˜

        WORK_DIR + 'pytorch_model.bin')



# ä½¿ç”¨ã—ãŸBertã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPytorchModelï¼‰ã‚’åˆ¥ã®ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜

shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')
gc.collect()

torch.cuda.empty_cache()

torch.cuda.memory_allocated()
from torch import nn

from torch.nn import functional as F

class NeuralNet(nn.Module):

    def __init__(self, num_aux_targets, sentence_feature_size):

        super(NeuralNet, self).__init__()

        # WORK_DIRä¸Šã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®BERTã‚’ä½œæˆ

        #self.bert = BertModel.from_pretrained(WORK_DIR)

        # bertãƒ¢ãƒ‡ãƒ«ã¯å­¦ç¿’ã—ãªã„

        #for param in self.bert.parameters():

            #param.requires_grad=False

            #print(f'bert-{param.requires_grad}')

        self.dropout = nn.Dropout(OUT_DROPOUT)

        

        # Bertã®ç‰¹å¾´ã‚’å­¦ç¿’

        self.before_linear = nn.Linear(BERT_HIDDEN_SIZE, BERT_HIDDEN_SIZE)

        self.before_linear2 = nn.Linear(BERT_HIDDEN_SIZE, 50)

        

        # sentence_featureã®ç‰¹å¾´ã‚’å­¦ç¿’

        self.sentence_feature_linear = nn.Linear(sentence_feature_size, sentence_feature_size)

        

        # Bertã¨sentence_featureã®æŠ±ãåˆã‚ã›ã‚’å­¦ç¿’

        n_hidden = 50 + sentence_feature_size

        self.mix_linear = nn.Linear(n_hidden, n_hidden)

        

        # å‡ºåŠ›

        self.linear_out = nn.Linear(n_hidden, 1)

        #nn.init.xavier_uniform_(self.linear_out.weight)

        self.linear_aux_out = nn.Linear(n_hidden, num_aux_targets)

        #nn.init.xavier_uniform_(self.linear_aux_out.weight)

        

    def forward(self, bert_output, sentence_feature):

        # encodeã‚»ãƒ«ã®æœ€åˆã®æ™‚ç³»åˆ—ï¼ˆClassificationï¼‰ä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„Bertãƒ¢ãƒ‡ãƒ«

        # å¤‰æ•°å‰ã®*ã¯ã€å¤‰æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹ï¼ˆå¼•æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‹ï¼Ÿï¼‰

        #_, bert_output = self.bert(*x_features, output_all_encoded_layers=False)

        bert_output = self.dropout(bert_output)

        

        bert_relu  = F.relu(bert_output)

        

        before_nn = self.before_linear(bert_relu)

        before_relu = F.relu(before_nn)

        before_nn2 = self.before_linear2(before_relu)

        before_relu2 = F.relu(before_nn2)

        

        sentence_feature_nn = self.sentence_feature_linear(sentence_feature)

        sentence_feature_relu = F.relu(sentence_feature_nn)

        

        h_cat = torch.cat((before_relu2, sentence_feature_relu), 1)

        mix_nn = self.mix_linear(h_cat)

        mix_relu = F.relu(mix_nn)

        

        result = self.linear_out(mix_relu)

        aux_result = self.linear_aux_out(mix_relu)

        out = torch.cat([result, aux_result], 1)

        

        return out
import math

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒæ¯ã«åˆ†ã‘ã‚‹ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿é–¢æ•°

class DynamicBucketIterator(object):

    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒæ¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ†å‰²

    def __init__(self, data, label, capacity, pad_token, shuffle, length_quantile, max_batch_size, for_bert):

        self.data = data

        self.label = label

        self.pad_token = pad_token

        self.capacity = capacity

        self.shuffle = shuffle

        self.length_quantile = length_quantile

        self.for_bert = for_bert

        

        # æ–‡ç« ãŒçŸ­ã„é †ã«æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®indexã‚’ã‚½ãƒ¼ãƒˆ

        self.index_sorted = sorted(range(len(self.data)), key=lambda i: len(self.data[i]))

        

        old_separator_index = 0

        self.separator_index_list = [0]

        for i_sample in range(len(self.data)):

            # æ–‡ç« ãŒçŸ­ã„é †ã«æ–‡ç« ãƒ‡ãƒ¼ã‚¿å–å¾—

            sample_index = self.index_sorted[i_sample]

            sample = self.data[sample_index]

            current_batch_size = i_sample - old_separator_index + 1

            if min(len(sample), MAX_LEN) * current_batch_size <= self.capacity and current_batch_size <= max_batch_size:

                pass

            else:

                # ãƒãƒƒãƒã®æœ€å¾Œã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®indexã‚’è¨˜éŒ²

                old_separator_index = i_sample

                self.separator_index_list.append(i_sample)

                

        # æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®æœ€å¾Œã®indexã‚’è¨˜éŒ²

        self.separator_index_list.append(len(self.data)) # [0, ..., start_separator_index, end_separator_index, ..., len(data)]

        

        if not self.shuffle:

            # ãƒãƒƒãƒæ•°å–å¾—

            self.bucket_index = range(self.__len__())

        

        self.reset_index()



    def reset_index(self):

        self.i_batch = 0

        

        if self.shuffle:

            self.index_sorted = sorted(np.random.permutation(len(self.data)), key=lambda i: len(self.data[i]))

            self.bucket_index = np.random.permutation(self.__len__())

    

    def __len__(self):

        return len(self.separator_index_list) - 1

    

    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿é–¢æ•°ã«ã‚ˆã‚Šå‘¼ã³å‡ºã•ã‚Œã‚‹é–¢æ•°

    def __iter__(self):

        return self

    

    # ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ã«ãƒãƒƒãƒæ¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™

    def __next__(self):

        # ãƒãƒƒãƒæ•°ãŒå…¨ã¦å‘¼ã³å‡ºã•ã‚Œã¦ã„ãŸã‚‰ã€åˆæœŸåŒ–ã—ã¦ã€ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†

        try:

            i_bucket = self.bucket_index[self.i_batch]

        except IndexError as e:

            self.reset_index()

            raise StopIteration

            

        start_index, end_index = self.separator_index_list[i_bucket : i_bucket + 2]

        

        # ãƒ‡ãƒ¼ã‚¿ã®indexã‚’ä½¿ç”¨é †ã«ä¿å­˜

        index_batch = self.index_sorted[start_index : end_index]



        raw_batch_data = [self.data[i] for i in index_batch]

        

        batch_label = self.label[index_batch]

        # ???

        math.ceil(1)

        

        # ãƒãƒƒãƒä¸­ã§ä¸€ç•ªé•·ã„æ–‡ç« ã®å˜èªæ•°ã‚’å–å¾—

        max_len = int(math.ceil(np.quantile([len(x) for x in raw_batch_data], self.length_quantile)))

        max_len = min([max_len, MAX_LEN])

        if max_len == 0:

            max_len = 1

        

        # BERTç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ•´å½¢ã—ã¦ã€è¿”ã™

        if self.for_bert:

            # ãƒãƒƒãƒã®æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®ç©ºé…åˆ—

            segment_id_batch = np.zeros((len(raw_batch_data), max_len))

            padded_batch = []

            input_mask_batch = []

            for sample in raw_batch_data:

                # ãƒãƒƒãƒå†…ã§æœ€é•·ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦ã€å˜èªãŒå…¥ã£ã¦ã‚‹æ‰€ã«1ã€å…¥ã£ã¦ãªã„æ‰€ã«0

                input_mask = [1] * len(sample) + [0] * (max_len - len(sample))

                input_mask_batch.append(input_mask[:max_len])



                # ãƒ‡ãƒ¼ã‚¿å†…ã‚’ãƒãƒƒãƒå†…ã§æœ€é•·ã®ãƒ‡ãƒ¼ã‚¿ã«åˆã‚ã›ã¦ã€0ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°

                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]

                padded_batch.append(sample[:max_len])



            self.i_batch += 1



            # ãƒãƒƒãƒæ¯ã«ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸæ–‡ç« ãƒ‡ãƒ¼ã‚¿ã€æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®ç©ºé…åˆ—ã€

            # æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®maskã€æ–‡ç« æ¯ã®å­¦ç¿’ã®æ­£è§£ãƒ‡ãƒ¼ã‚¿ã€æ–‡ç« ãƒ‡ãƒ¼ã‚¿ã®indexã‚’è¿”ã™

            return padded_batch, segment_id_batch, input_mask_batch, batch_label, index_batch

        

        else:

            padded_batch = []

            for sample in raw_batch_data:

                sample = sample + [self.pad_token for _ in range(max_len - len(sample))]

                padded_batch.append(sample[:max_len])



            self.i_batch += 1



            return padded_batch, batch_label, index_batch

        

def sigmoid(x):

    return np.where(x<-709.0, 0.0, 1 / (1 + np.exp(-x)))
import time

import torch.optim as optim



test_dict = {}

fold_list = [0]

epochs = 7



start_time = time.time()



# encodeã‚»ãƒ«ã®æœ€åˆã®æ™‚ç³»åˆ—ï¼ˆClassificationï¼‰ä»¥å¤–ã¯å‡ºåŠ›ã—ãªã„Bertãƒ¢ãƒ‡ãƒ«

# WORK_DIRä¸Šã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®BERTã‚’ä½œæˆ

bert = BertModel.from_pretrained(WORK_DIR).cuda()

#for param in bert.parameters():

    #param.requires_grad=False

    #print(f'bert-{param.requires_grad}')





model = NeuralNet(y_label_size-1, sentence_feature_size)

# æœ€é©åŒ–æ‰‹æ³•ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)

model.cuda()



# ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒ1ã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¹é–¢æ•°ï¼ˆBCEWithLogitsLossï¼‰ã¸ã®å½±éŸ¿åº¦

#pos_weight = (len(y_targetlabel) - y_target.sum(0)) / y_target.sum(0)

#pos_weight[pos_weight == float("Inf")] = 1

#print(pos_weight)

loss_fn=nn.BCEWithLogitsLoss(reduction='mean')#, pos_weight=pos_weight)



#for param in model.parameters():

    #print(param.requires_grad)



highest_accuracy = 0

lowest_loss = len(valid) * 100

#print(lowest_loss)

for i_fold in fold_list:

    fold_start_time = time.time()



    train_loader = DynamicBucketIterator(train, 

                                        train_label,

                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)

    

    #print(valid_label)

    valid_loader = DynamicBucketIterator(valid, 

                                         valid_label,

                                         capacity=MAX_LEN*batch_size,

                                         pad_token=0,

                                         shuffle=False,

                                         length_quantile=1,

                                         max_batch_size=2048,

                                         for_bert=True)



    print(torch.cuda.memory_allocated())



    eval_start_time = time.time()

    for epoch in range(epochs):

        batch_i = 0

        train_loss_validation = 0

        model.train()

        for batch in train_loader:

            x_batch = batch[0]

            segment_id_batch = batch[1]

            input_mask_batch = batch[2]

            y_batch = batch[3]

            #print(y_batch)

            y_targets_batch = y_batch[:, :y_label_size]

            sentence_feature = y_batch[:, -sentence_feature_size:]

            #print(y_targets_batch)

            #print(sentence_feature)

            index_batch = batch[4]

            #sample_weight_batch = y_batch[:, len(y_batch[1])-1]

            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]



            with torch.no_grad():

                # å¤‰æ•°å‰ã®*ã¯ã€å¤‰æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹ï¼ˆå¼•æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‹ï¼Ÿï¼‰

                _, bert_output = bert(*x_features, output_all_encoded_layers=False)

            #print(bert_output.grad_fn)

            # å‹¾é…ã®åˆæœŸåŒ–

            optimizer.zero_grad()

            out = model(bert_output, sentence_feature)

            #print(model.linear_out.weight.grad)

            #print(y_targets_batch)

            test_dict[f'{epoch}-{batch_i}'] = sigmoid(out.detach().cpu().numpy())

            batch_i += 1

            

            # ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒ1ã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¹é–¢æ•°ï¼ˆBCEWithLogitsLossï¼‰ã¸ã®å½±éŸ¿åº¦

            #pos_weight = (len(y_targets_batch) - y_targets_batch.sum(0)) / y_targets_batch.sum(0)

            #pos_weight[pos_weight == float("Inf")] = 1

            #print(pos_weight)

            #loss_fn=nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)

            

            loss = loss_fn(out, y_targets_batch)

            #print(f'{epoch}-{batch_i}:{loss.item() / len(y_batch)}')

            

            train_loss_validation += loss.item()

            # å‹¾é…ã®è¨ˆç®—

            loss.backward()

            #print(model.linear_out.weight.grad)

            #print(bert_output.grad_fn.grad)

            

            # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°

            optimizer.step()

            del x_batch, segment_id_batch, input_mask_batch, y_batch, y_targets_batch, sentence_feature, index_batch, x_features, out, loss, bert_output

            torch.cuda.empty_cache()

        print("train_loss_validation:", train_loss_validation)

        

        valid_pred = np.zeros(len(valid))

        batch_i = 0

        loss_validation = 0

        model.eval()

        for batch in valid_loader:

            x_batch = batch[0]

            segment_id_batch = batch[1]

            input_mask_batch = batch[2]

            y_batch = batch[3]

            #print(y_batch)

            y_targets_batch = y_batch[:, :y_label_size]

            sentence_feature = y_batch[:, -sentence_feature_size:]

            #print(y_targets_batch)

            #print(sentence_feature)

            index_batch = batch[4]

            x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]



            with torch.no_grad():

                # å¤‰æ•°å‰ã®*ã¯ã€å¤‰æ•°ã‚’ã‚¢ãƒ³ãƒ‘ãƒƒã‚¯ã™ã‚‹ï¼ˆå¼•æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‹ï¼Ÿï¼‰

                _, bert_output = bert(*x_features, output_all_encoded_layers=False)

            #print(bert_output.grad_fn)

            

            y_pred = model(bert_output, sentence_feature)

            

            #print("y_pred:", y_pred[:, 0])

            #print("y_batch:", y_batch[:, 0])

            # ã‚¯ãƒ©ã‚¹åˆ†é¡ãŒ1ã®ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¹é–¢æ•°ï¼ˆBCEWithLogitsLossï¼‰ã¸ã®å½±éŸ¿åº¦

            #pos_weight = (len(y_targets_batch) - y_targets_batch.sum(0)) / y_targets_batch.sum(0)

            #pos_weight[pos_weight == float("Inf")] = 1

            #print(pos_weight)

            #loss_fn=nn.BCEWithLogitsLoss(reduction='mean', pos_weight=pos_weight)

            

            loss = loss_fn(y_pred, y_targets_batch)

            

            loss_validation += loss.item()

            #print(y_batch)

            #test_dict[f'{epoch}-{batch_i}'] = sigmoid(out.detach().cpu().numpy())

            batch_i += 1

            

            valid_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())

            

            del x_batch, segment_id_batch, input_mask_batch, y_batch, y_targets_batch, sentence_feature, index_batch, x_features#, loss_fn

            torch.cuda.empty_cache()

        

        valid_pred = (torch.from_numpy(valid_pred) >= 0.5).to(torch.float32).cuda()

        correct = (valid_pred == valid_label[:, 0]).to(torch.float32).sum(0)

        positive_correct = ((valid_pred == valid_label[:, 0]) & (valid_label[:, 0] == 1.0)).to(torch.float32).sum(0)

        negative_correct = correct - positive_correct

        accuracy = correct / len(valid_label)

        positive_sum = valid_label[:, 0].sum(0)

        positive_accuracy = positive_correct / positive_sum

        negative_accuracy = negative_correct / (len(valid_label) - positive_sum)

        if (accuracy >= highest_accuracy) | (lowest_loss >= loss_validation):

            print(epoch)

            print(accuracy)

            print(positive_accuracy)

            print(negative_accuracy)

            highest_accuracy = accuracy

            lowest_loss = loss_validation

            print("lowest_loss:", lowest_loss)

            # é‡ã¿ã‚„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å€¤ã®ã¿ä¿å­˜ï¼ˆä¿å­˜ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªç•ªåœ°ãªã©ã¯ä¿å­˜ã—ãªã„ï¼‰

            torch.save(model.state_dict(), '../fine-tuning')

            

        print("loss_validation:", loss_validation)

        print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))



    del train_loader, valid_loader, correct, accuracy, batch_i, positive_correct, negative_correct, positive_sum, positive_accuracy, negative_accuracy

    gc.collect()

    torch.cuda.empty_cache()
del train, valid, valid_label, train_label, optimizer, loss_fn

gc.collect()

torch.cuda.empty_cache()



print(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))
#test_dict
test_dict = {}

fold_list = [0]

#min_out = torch.tensor(-709, dtype=torch.float32).cuda()



i_epoch = 2

start_time = time.time()



model.load_state_dict(torch.load('../fine-tuning'))

model.eval()

#for param in model.parameters():

    #print(param.requires_grad)



for i_fold in fold_list:

    fold_start_time = time.time()



    test_loader = DynamicBucketIterator(test, 

                                        test_label,

                                        capacity=MAX_LEN*batch_size, pad_token=0, shuffle=False, length_quantile=1, max_batch_size=2048, for_bert=True)



    print(torch.cuda.memory_allocated())



    test_pred = np.zeros(len(test))

    eval_start_time = time.time()

    for batch in test_loader:

        x_batch = batch[0]

        segment_id_batch = batch[1]

        input_mask_batch = batch[2]

        y_batch = batch[3]

        sentence_feature = y_batch[:, -sentence_feature_size:]

        index_batch = batch[4]

        x_features = [torch.tensor(feature, dtype=torch.long).cuda() for feature in [x_batch, segment_id_batch, input_mask_batch]]

        #                 print('x_features', torch.cuda.memory_allocated())

        

        with torch.no_grad():

            _, bert_output = bert(*x_features, output_all_encoded_layers=False)

        #print(bert_output.grad_fn)

        

        y_pred = model(bert_output, sentence_feature)

        #                 print('after_prediction', torch.cuda.memory_allocated())

        #y_pred = torch.where(y_pred < min_out, min_out, y_pred)

        #print(y_pred)

        test_pred[index_batch] = sigmoid(y_pred[:, 0].detach().cpu().numpy())

        del x_batch, segment_id_batch, input_mask_batch, index_batch, y_batch, sentence_feature, x_features, y_pred, bert_output

        torch.cuda.empty_cache()

    print(f'uncased {i_fold} finishd in {time.time() - fold_start_time}', file=open('time.txt', 'a'))



    test_dict[i_fold] = test_pred

    #print(epoch_test_pred)

    del model, test_loader

    gc.collect()

    torch.cuda.empty_cache()
test_pred = (torch.from_numpy(test_pred) >= 0.5).to(torch.float32).cuda()

correct = (test_pred == test_label[:, 0]).to(torch.float32).sum(0)

accuracy = correct / len(test_label)

print(accuracy)



del accuracy, correct, test_pred

gc.collect()

torch.cuda.empty_cache()

print(f'uncased finishd in {time.time() - start_time}', file=open('time.txt', 'a'))
#min_out = torch.tensor(-700, dtype=torch.float32)

#y_pred = torch.tensor([-709, -710], dtype=torch.float32)

#pred = torch.where(y_pred < min_out, min_out, y_pred)

#sigmoid(pred.detach().cpu().numpy())
#test_dict
#df_submit = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv')

#df_submit.prediction = test_dict[0]

#df_submit.to_csv('submission.csv', index=False)