
import pandas as pd

import numpy as np



from sklearn.model_selection import cross_val_predict

from sklearn.model_selection import StratifiedKFold

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import log_loss, accuracy_score

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.svm import SVC

from sklearn.decomposition import TruncatedSVD

from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import LabelEncoder



import gensim



import scikitplot.plotters as skplt



import nltk



from xgboost import XGBClassifier



import os



from keras.preprocessing.text import Tokenizer

from keras.preprocessing.sequence import pad_sequences

from keras.models import Sequential

from keras.layers import Dense, Embedding, LSTM, ConvLSTM2D

from keras.utils.np_utils import to_categorical

from keras.callbacks import ModelCheckpoint

from keras.models import load_model

from keras import regularizers

from keras.constraints import max_norm, unit_norm

from keras.constraints import non_neg

from keras.optimizers import Adam
df_train_txt = pd.read_csv('../input/training_text', sep='\|\|', header=None, skiprows=1, names=["ID","Text"])

df_train_txt.head()
df_train_var = pd.read_csv('../input/training_variants')

df_train_var.head()
df_test_txt = pd.read_csv('../input/test_text', sep='\|\|', header=None, skiprows=1, names=["ID","Text"])

df_test_txt.head()
df_test_var = pd.read_csv('../input/test_variants')

df_test_var.head()
df_train = pd.merge(df_train_var, df_train_txt, how='left', on='ID')

df_train.head()
df_test = pd.merge(df_test_var, df_test_txt, how='left', on='ID')

df_test.head()
df_train.describe(include='all')
df_test.describe(include='all')
df_train['Class'].value_counts().plot(kind="bar", rot=0)
# This cell reduces the training data for Kaggle limits. Remove this cell for real results.

df_train, _ = train_test_split(df_train, test_size=0.7, random_state=8, stratify=df_train['Class'])

df_train.shape
def evaluate_features(X, y, clf=None):

    """General helper function for evaluating effectiveness of passed features in ML model

    

    Prints out Log loss, accuracy, and confusion matrix with 3-fold stratified cross-validation

    

    Args:

        X (array-like): Features array. Shape (n_samples, n_features)

        

        y (array-like): Labels array. Shape (n_samples,)

        

        clf: Classifier to use. If None, default Log reg is use.

    """

    if clf is None:

        clf = LogisticRegression()

    

    probas = cross_val_predict(clf, X, y, cv=StratifiedKFold(random_state=8), 

                              n_jobs=-1, method='predict_proba', verbose=2)

    pred_indices = np.argmax(probas, axis=1)

    classes = np.unique(y)

    preds = classes[pred_indices]

    print('Log loss: {}'.format(log_loss(y, probas)))

    print('Accuracy: {}'.format(accuracy_score(y, preds)))

    skplt.plot_confusion_matrix(y, preds)

class MySentences(object):

    """MySentences is a generator to produce a list of tokenized sentences 

    

    Takes a list of numpy arrays containing documents.

    

    Args:

        arrays: List of arrays, where each element in the array contains a document.

    """

    def __init__(self, *arrays):

        self.arrays = arrays

 

    def __iter__(self):

        for array in self.arrays:

            for document in array:

                for sent in nltk.sent_tokenize(document):

                    yield nltk.word_tokenize(sent)



def get_word2vec(sentences, location):

    """Returns trained word2vec

    

    Args:

        sentences: iterator for sentences

        

        location (str): Path to save/load word2vec

    """

    if os.path.exists(location):

        print('Found {}'.format(location))

        model = gensim.models.Word2Vec.load(location)

        return model

    

    print('{} not found. training model'.format(location))

    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)

    print('Model done training. Saving to disk')

    model.save(location)

    return model
w2vec = get_word2vec(

    MySentences(

        df_train['Text'].values, 

        #df_test['Text'].values  Commented for Kaggle limits

    ),

    'w2vmodel'

)
class MyTokenizer:

    def __init__(self):

        pass

    

    def fit(self, X, y=None):

        return self

    

    def transform(self, X):

        transformed_X = []

        for document in X:

            tokenized_doc = []

            for sent in nltk.sent_tokenize(document):

                tokenized_doc += nltk.word_tokenize(sent)

            transformed_X.append(np.array(tokenized_doc))

        return np.array(transformed_X)

    

    def fit_transform(self, X, y=None):

        return self.transform(X)



class MeanEmbeddingVectorizer(object):

    def __init__(self, word2vec):

        self.word2vec = word2vec

        # if a text is empty we should return a vector of zeros

        # with the same dimensionality as all the other vectors

        self.dim = len(word2vec.wv.syn0[0])



    def fit(self, X, y=None):

        return self



    def transform(self, X):

        X = MyTokenizer().fit_transform(X)

        

        return np.array([

            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]

                    or [np.zeros(self.dim)], axis=0)

            for words in X

        ])

    

    def fit_transform(self, X, y=None):

        return self.transform(X)

mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)

mean_embedded = mean_embedding_vectorizer.fit_transform(df_train['Text'])
evaluate_features(mean_embedded, df_train['Class'].values.ravel())
evaluate_features(mean_embedded, df_train['Class'].values.ravel(),

                  RandomForestClassifier(n_estimators=1000, max_depth=15, verbose=1))
evaluate_features(mean_embedded, 

                  df_train['Class'].values.ravel(),

                  XGBClassifier(max_depth=4,

                                objective='multi:softprob',

                                learning_rate=0.03333,

                                )

                 )
# Use the Keras tokenizer

num_words = 2000

tokenizer = Tokenizer(num_words=num_words)

tokenizer.fit_on_texts(df_train['Text'].values)
# Pad the data 

X = tokenizer.texts_to_sequences(df_train['Text'].values)

X = pad_sequences(X, maxlen=2000)
#print(mean_embedded.shape)
# Build out our simple LSTM

embed_dim = 128

lstm_out = 196



# Model saving callback

ckpt_callback = ModelCheckpoint('keras_model', 

                                 monitor='val_loss', 

                                 verbose=1, 

                                 save_best_only=True, 

                                 mode='auto')



model = Sequential()

model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))

model.add(LSTM(lstm_out, recurrent_dropout=0.2, dropout=0.2, kernel_regularizer=regularizers.l2(0.01), recurrent_regularizer = regularizers.l2(0.01), activity_regularizer = regularizers.l2(0.01), kernel_constraint = unit_norm(axis=0)))

model.add(Dense(9,activation='softmax'))

model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['categorical_crossentropy'])

print(model.summary())
Y = pd.get_dummies(df_train['Class']).values

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)

print(X_train.shape, Y_train.shape)

print(X_test.shape, Y_test.shape)

batch_size = 32

model.fit(X_train, Y_train, epochs=8, batch_size=batch_size, validation_split=0.2, callbacks=[ckpt_callback])
model = load_model('keras_model')
probas = model.predict(X_test)
pred_indices = np.argmax(probas, axis=1)

classes = np.array(range(1, 10))

preds = classes[pred_indices]

print('Log loss: {}'.format(log_loss(classes[np.argmax(Y_test, axis=1)], probas)))

print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(Y_test, axis=1)], preds)))

skplt.plot_confusion_matrix(classes[np.argmax(Y_test, axis=1)], preds)

gene_le = LabelEncoder()

gene_encoded = gene_le.fit_transform(df_train['Gene'].values.ravel()).reshape(-1, 1)

gene_encoded = gene_encoded / np.max(gene_encoded)
variation_le = LabelEncoder()

variation_encoded = variation_le.fit_transform(df_train['Variation'].values.ravel()).reshape(-1, 1)

variation_encoded = variation_encoded / np.max(variation_encoded)
evaluate_features(np.hstack((gene_encoded, variation_encoded, truncated_tfidf)), df_train['Class'])
evaluate_features(np.hstack((gene_encoded, variation_encoded, truncated_tfidf)), df_train['Class'],

                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))
evaluate_features(np.hstack((gene_encoded, variation_encoded, mean_embedded)), df_train['Class'])
evaluate_features(np.hstack((gene_encoded, variation_encoded, mean_embedded)), df_train['Class'],

                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))
one_hot_gene = pd.get_dummies(df_train['Gene'])

svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)

truncated_one_hot_gene = svd.fit_transform(one_hot_gene.values)
one_hot_variation = pd.get_dummies(df_train['Variation'])

svd = TruncatedSVD(n_components=25, n_iter=25, random_state=12)

truncated_one_hot_variation = svd.fit_transform(one_hot_variation.values)
evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, truncated_tfidf)), df_train['Class'])
evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, truncated_tfidf)), df_train['Class'],

                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))

evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, mean_embedded)), df_train['Class'])
evaluate_features(np.hstack((truncated_one_hot_gene, truncated_one_hot_variation, mean_embedded)), df_train['Class'],

                  RandomForestClassifier(n_estimators=1000, max_depth=5, verbose=1))
lr_w2vec = LogisticRegression()

lr_w2vec.fit(mean_embedded, df_train['Class'])
mean_embedded_test = mean_embedding_vectorizer.transform(df_test['Text'])
probas = lr_w2vec.predict_proba(mean_embedded_test)
submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])

submission_df['ID'] = df_test['ID']

submission_df.head()
submission_df.to_csv('submission.csv', index=False)
xgb_w2vec = XGBClassifier(max_depth=4,

                          objective='multi:softprob',

                          learning_rate=0.03333)

xgb_w2vec.fit(mean_embedded, df_train['Class'])

probas = xgb_w2vec.predict_proba(mean_embedded_test)

submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])

submission_df['ID'] = df_test['ID']

submission_df.to_csv('submission.csv', index=False)
svc_w2vec = SVC(kernel='linear', probability=True)

svc_w2vec.fit(mean_embedded, df_train['Class'])

probas = svc_w2vec.predict_proba(mean_embedded_test)

submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])

submission_df['ID'] = df_test['ID']

submission_df.to_csv('submission.csv', index=False)
Xtest = tokenizer.texts_to_sequences(df_test['Text'].values)

Xtest = pad_sequences(Xtest, maxlen=2000)
probas = model.predict(Xtest)
submission_df = pd.DataFrame(probas, columns=['class'+str(c+1) for c in range(9)])

submission_df['ID'] = df_test['ID']

submission_df.head()
submission_df.to_csv('submission.csv', index=False)